{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Interconnect-Churn\" data-toc-modified-id=\"Interconnect-Churn-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><a id=\"toc1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Interconnect Churn</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a id=\"toc1_1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Purpose</a></a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><a id=\"toc1_2_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Feature Engineering</a></a></span></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><a id=\"toc1_3_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Clustering</a></a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><a id=\"toc1_4_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Modeling</a></a></span></li><li><span><a href=\"#Final-Work-Plan\" data-toc-modified-id=\"Final-Work-Plan-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span><a id=\"toc1_5_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Final Work Plan</a></a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span><a id=\"toc1_6_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Feature Engineering</a></a></span></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span><a id=\"toc1_7_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">EDA</a></a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span><a id=\"toc1_8_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Preprocessing</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Means-Clustering\" data-toc-modified-id=\"K-Means-Clustering-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span><a id=\"toc5_1_1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">K Means Clustering</a></a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span><a id=\"toc5_2_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Modeling</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Classifier-Pipeline\" data-toc-modified-id=\"Classifier-Pipeline-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span><a id=\"toc5_2_1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Classifier Pipeline</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#ADA-Pipeline-Feature-Importance\" data-toc-modified-id=\"ADA-Pipeline-Feature-Importance-1.9.1.1\"><span class=\"toc-item-num\">1.9.1.1&nbsp;&nbsp;</span><a id=\"toc5_2_1_1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">ADA Pipeline Feature Importance</a></a></span></li><li><span><a href=\"#XGBoost-Pipeline-Feature-Importance\" data-toc-modified-id=\"XGBoost-Pipeline-Feature-Importance-1.9.1.2\"><span class=\"toc-item-num\">1.9.1.2&nbsp;&nbsp;</span><a id=\"toc5_2_1_2_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">XGBoost Pipeline Feature Importance</a></a></span></li><li><span><a href=\"#LGBM-Pipeline-Feature-Importance\" data-toc-modified-id=\"LGBM-Pipeline-Feature-Importance-1.9.1.3\"><span class=\"toc-item-num\">1.9.1.3&nbsp;&nbsp;</span><a id=\"toc5_2_1_3_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">LGBM Pipeline Feature Importance</a></a></span></li><li><span><a href=\"#Gradient-Boost-Pipeline-Feature-Importance\" data-toc-modified-id=\"Gradient-Boost-Pipeline-Feature-Importance-1.9.1.4\"><span class=\"toc-item-num\">1.9.1.4&nbsp;&nbsp;</span><a id=\"toc5_2_1_4_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Gradient Boost Pipeline Feature Importance</a></a></span></li><li><span><a href=\"#CatBoost-Pipeline-Feature-Importance\" data-toc-modified-id=\"CatBoost-Pipeline-Feature-Importance-1.9.1.5\"><span class=\"toc-item-num\">1.9.1.5&nbsp;&nbsp;</span><a id=\"toc5_2_1_5_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">CatBoost Pipeline Feature Importance</a></a></span></li></ul></li><li><span><a href=\"#Comparing-Models\" data-toc-modified-id=\"Comparing-Models-1.9.2\"><span class=\"toc-item-num\">1.9.2&nbsp;&nbsp;</span><a id=\"toc5_2_2_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Comparing Models</a></a></span></li></ul></li><li><span><a href=\"#Final-Model\" data-toc-modified-id=\"Final-Model-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span><a id=\"toc5_3_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Final Model</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Final-Model-Evaluation-Metric\" data-toc-modified-id=\"Final-Model-Evaluation-Metric-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span><a id=\"toc5_3_1_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Final Model Evaluation Metric</a></a></span></li><li><span><a href=\"#Final-Model-Feature-Importances\" data-toc-modified-id=\"Final-Model-Feature-Importances-1.10.2\"><span class=\"toc-item-num\">1.10.2&nbsp;&nbsp;</span><a id=\"toc5_3_2_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Final Model Feature Importances</a></a></span></li></ul></li><li><span><a href=\"#Overall-Conclusions\" data-toc-modified-id=\"Overall-Conclusions-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span><a id=\"toc5_4_\" rel=\"nofollow\"></a><a href=\"#toc0_\" rel=\"nofollow\">Overall Conclusions</a></a></span></li><li><span><a href=\"#Solution-Report\" data-toc-modified-id=\"Solution-Report-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Solution Report</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Interconnect Churn](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Purpose](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The telecom operator Interconnect would like to forecast churn of their clients. To ensure loyalty, those who are predicted to leave will be offered promotional codes and special plans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ydata_profiling  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\xix\\anaconda3\\envs\\streamlit\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ydata_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly_express\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m  \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mydata_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrdinalEncoder\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ydata_profiling'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import plotly_express as px  \n",
    "import seaborn as sns\n",
    "from ydata_profiling.profile_report import ProfileReport\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from catboost import CatBoostClassifier, Pool, cv \n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "contract = pd.read_csv('datasets/contract.csv', parse_dates=[1, 2])\n",
    "internet = pd.read_csv('datasets/internet.csv')\n",
    "personal = pd.read_csv('datasets/personal.csv')\n",
    "phone = pd.read_csv('datasets/phone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA profile reports\n",
    "con_prof = ProfileReport(contract, title= 'Contract Profile')\n",
    "int_prof = ProfileReport(internet, title= 'Internet Profile')\n",
    "per_prof = ProfileReport(personal, title= 'Personal Profile')\n",
    "pho_prof = ProfileReport(phone, title= 'Contract Profile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract profile report\n",
    "con_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contract dataset contains a few features, and the target variable in the form of end date. If the customer has churned, there will be an end date, otherwise the customer is still with the company. Therefore, we can see class imbalance, which will need to be rectified to improve the score of the model. The column names will need to be changed to lowercase, and underscores will be added to separate words. Customer Id is a common column among all the datasets, therefore we will use that to combine them. Customer id contains 7043 values, and all are unique. Of note, begin date has moderate correlation with type. This dataset does not contain missing values, ut has categorical columns that may need encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internet profile report\n",
    "int_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internet dataset contains a customer id column with a few other categorical features. The dataset contains 5517 observations with no missing values. This profile report also shows us the distributions of the different features. Most of the features have two distinct values, the absence or presence of a service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personal profile report\n",
    "per_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA shows us that gender among the personal dataset is even. A vast majority of customers are not senior citizens, and the distribution of having a partner is fairly even. A majority of customers have dependents. The dataset is free of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone profile report\n",
    "pho_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has one column that has a yes or no value for multiple lines, paired with customer id. The multiple lines feature is fairly balanced with no values missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different datasets will be combined into one full dataset by joining on the column id. The dates column will be parsed to create new features. Furthermore, the end date column will be label encoded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Clustering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis will be done to se if the the full dataset contains clusters of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Modeling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some boosting models work with categorical features natively, but we may have to label encode those features. We will also need to scale the data, and balance the classes for churn by upsampling. We will then test across a couple of models, and optimize the AUCROC score. Finally, we will see if a voting classifier improves the AUCROC further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Final Work Plan](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read data\n",
    "2. Combine data\n",
    "3. Feature engineering/ change column names and types\n",
    "4. EDA\n",
    "5. KNN clustering\n",
    "6. Preprocessing, upscaling, standard scaling\n",
    "7. Modeling with boost methods and cross validation\n",
    "8. Hyperparameter tuning\n",
    "9. Feature importance\n",
    "10. Visualizing modeling results \n",
    "11. Voting Classifier \n",
    "12. Visualize final model, feature importance\n",
    "13. Conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Feature Engineering](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contract columns\n",
    "contract.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names\n",
    "contract.columns = ['customer_id', 'begin_date', 'end_date', 'type', 'paperless_billing', 'payment_method', 'monthly_charges', 'total_charges']\n",
    "internet.columns = [ 'customer_id', 'internet_service', 'online_security', 'online_backup', 'device_protection', 'tech_support', 'streaming_tv', 'streaming_movies']\n",
    "personal.columns = ['customer_id', 'gender', 'senior_citizen', 'partner', 'dependents']\n",
    "phone.columns = ['customer_id', 'multiple_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new contract columns\n",
    "contract.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new internet columns\n",
    "internet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new personal columns\n",
    "personal.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new phone columns\n",
    "phone.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first merge\n",
    "df2 = contract.merge(internet, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of first merge \n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second merge\n",
    "df3 = df2.merge(personal, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second merge results\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final merge\n",
    "df = df3.merge(phone, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on the columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting new features from begin date\n",
    "df['year'] = df.begin_date.dt.year\n",
    "df['month'] = df.begin_date.dt.month\n",
    "df['day'] = df.begin_date.dt.day\n",
    "df['quarter'] = df.begin_date.dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create churn column\n",
    "df['churn'] = np.where(df['end_date'] == 'No', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ed_date for churn \n",
    "df.drop(columns=['end_date', 'customer_id', 'begin_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with No, representing no service\n",
    "df.fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck for missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we found empty strings\n",
    "df.query(\"total_charges==' '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values\n",
    "df.query(\"total_charges==' '\")['total_charges'].count()/len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than 0.2 % of the data has missing total charges.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those rows where total charges is an empty string\n",
    "df = df.drop(df[df['total_charges'] == ' '].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we thought to fill in these missing total charges with zero, but that would mean we made an inference that these customers did not use any services during their contract. We could have tried imputing the values, but since it represents only 0.2% of the data, we will drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking proper implementation\n",
    "df.query(\"total_charges==' '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert total charges to float\n",
    "df.total_charges = pd.to_numeric(df.total_charges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at dtypes of columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a few features to the dataset by parsing the dates, and separating them into their individual components. We then converted the end date column into a binary column named churn. After combining the datasets, we introduced missing values. We dropped those missing values so we could convert total charges into a float. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[EDA](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summart stats \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target values\n",
    "df.churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see summary statistics on the full dataset. Looking at correlations, we see the begin date year has the strongest correlation with churn amongst the other features, yet is is a weak corelation. The monthly charges have a moderate negative correlation with the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Preprocessing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical columns\n",
    "columns = ['type', 'paperless_billing',\n",
    "       'payment_method',\n",
    "       'internet_service', 'online_security', 'online_backup',\n",
    "       'device_protection', 'tech_support', 'streaming_tv', 'streaming_movies',\n",
    "       'gender', 'senior_citizen', 'partner', 'dependents', 'multiple_lines']\n",
    "encoder = OrdinalEncoder()\n",
    "df_ordinal = pd.DataFrame(encoder.fit_transform(df[columns]), columns=df[columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal dataset\n",
    "df_ordinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace categorical columns with ordinal columns\n",
    "df[columns] = df_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at resulting dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target\n",
    "X = df.drop(columns='churn')\n",
    "y = df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target value counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at training features\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we encoding all the categorical columns with label encoding. we followed up by separating the data into features and targets. We then noticed the class imbalance in churn, sow e used upsampling to rectify this. Then, we concluded preprocessing with the standard scaling on the numerical columns. Looking at our final training dataset, we are prepared for modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_1_'></a>[K Means Clustering](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy of df\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2, random_state=19)\n",
    "km.fit(df_copy)\n",
    "\n",
    "print('Cluster centroids:')\n",
    "print(km.cluster_centers_)\n",
    "print()\n",
    "print(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot \n",
    "\n",
    "centroids = pd.DataFrame(km.cluster_centers_, columns=df_copy.columns)\n",
    "df_copy['label'] = km.labels_.astype(str)\n",
    "centroids['label'] = ['0 centroid', '1 centroid']\n",
    "data_all = pd.concat([df_copy, centroids], ignore_index=True)\n",
    "\n",
    "# Plot the graph\n",
    "sns.pairplot(data_all, hue='label', diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the pairplot, we can clearly see the two clusters in various category pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Modeling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Classifier Pipeline](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K fold cross val\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially used K-fold cross validation for hyperparameter tuning using gridsearch cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier pipeline\n",
    "pipe_ada = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('ada_classifier', AdaBoostClassifier(n_estimators=500,  random_state=19, learning_rate=0.1))])\n",
    "\n",
    "pipe_xg = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('xg_classifier', XGBClassifier(learning_rate=0.5, max_depth=5, n_estimators=1500))])\n",
    "\n",
    "pipe_lgbm = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('lgbm_classifier', lgb.LGBMClassifier(num_boost_round=500, objective='binary', metric='auc', random_state=19, learning_rate=0.1, n_estimators=350, num_leaves=30, max_depth=20, boosting_type='gbdt'))])\n",
    "\n",
    "pipe_gb = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('gb_classifier', GradientBoostingClassifier(n_estimators=1500, learning_rate=0.1, random_state=19, max_depth=5))])\n",
    "\n",
    "pipe_cat = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('cat_classifier', CatBoostClassifier(task_type='GPU', loss_function='Logloss', eval_metric='AUC', iterations=1500, random_seed=19))])\n",
    "\n",
    "pipe_vote = Pipeline([('scalar1', StandardScaler()), ('oversampler1', SMOTE(random_state=19)), ('vote_classifier', VotingClassifier(estimators=[('ada', pipe_ada), ('xgb', pipe_xg), ('lgb', pipe_lgbm), ('cat', pipe_cat)], verbose=1, voting='soft'))])\n",
    "\n",
    "pipelines = [pipe_ada, pipe_xg, pipe_lgbm, pipe_gb, pipe_cat, pipe_vote]\n",
    "\n",
    "best_auc = 0\n",
    "best_classifier = 0\n",
    "best_pipeline = \"\"\n",
    "\n",
    "pipe_dict = {0: 'ADA Boost', 1: 'XG Boost', 2: 'Light GBM', 3: 'Gradient Boost', 4: 'Cat Boost', 5: 'Voting Classifier'}\n",
    "\n",
    "# Use cross-validation to evaluate the models\n",
    "for i, model in enumerate(pipelines):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print('{} Cross-Validation AUC-ROC: {:.2f}'.format(pipe_dict[i], scores.mean()))\n",
    "    if scores.mean() > best_auc:\n",
    "        best_auc = scores.mean()\n",
    "        best_pipeline = model\n",
    "        best_classifier = i\n",
    "\n",
    "# Print the best classifier\n",
    "print('\\nClassifier with the best AUC-ROC: {}'.format(pipe_dict[best_classifier]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a pipeline to determine the best classifier to use. We used a standard scaler and SMOTE for preprocessing. The models with the best cross validation scores on the training set was the voting classifier and XG Boost. Therefore, we will implement the voting classifier and measure the model against the test data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_1_'></a>[ADA Pipeline Feature Importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada pipeline feature importance\n",
    "pipe_ada.fit(X_train, y_train)\n",
    "\n",
    "ada_classifier = pipe_ada.named_steps['ada_classifier']\n",
    "ada_importances = ada_classifier.feature_importances_\n",
    "ada_indices = np.argsort(ada_importances)[::-1]\n",
    "\n",
    "top_10_features = []\n",
    "for f in range(10):\n",
    "    feature_index = ada_indices[f]\n",
    "    feature_name = X_train.columns[feature_index]\n",
    "    top_10_features.append((feature_name, ada_importances[feature_index]))\n",
    "\n",
    "ada_top_10_df = pd.DataFrame(top_10_features, columns=['Feature', 'Importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA feature imprtance\n",
    "px.pie(ada_top_10_df, names='Feature', values='Importance', title='ADA Boost Feature Importance', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_2_'></a>[XGBoost Pipeline Feature Importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Boost pipeline feature importance\n",
    "pipe_xg.fit(X_train, y_train)\n",
    "\n",
    "xg_classifier = pipe_xg.named_steps['xg_classifier']\n",
    "xg_importances = xg_classifier.feature_importances_\n",
    "xg_indices = np.argsort(xg_importances)[::-1]\n",
    "\n",
    "top_10_features = []\n",
    "for f in range(10):\n",
    "    feature_index = xg_indices[f]\n",
    "    feature_name = X_train.columns[feature_index]\n",
    "    top_10_features.append((feature_name, xg_importances[feature_index]))\n",
    "\n",
    "xg_top_10_df = pd.DataFrame(top_10_features, columns=['Feature', 'Importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB feature importance\n",
    "px.pie(xg_top_10_df, names='Feature', values='Importance', title='XG Boost Feature Importance', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_3_'></a>[LGBM Pipeline Feature Importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light GBM pipeline feature importance\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "\n",
    "lg_classifier = pipe_lgbm.named_steps['lgbm_classifier']\n",
    "lg_importances = lg_classifier.feature_importances_\n",
    "lg_indices = np.argsort(lg_importances)[::-1]\n",
    "\n",
    "top_10_features = []\n",
    "for f in range(10):\n",
    "    feature_index = lg_indices[f]\n",
    "    feature_name = X_train.columns[feature_index]\n",
    "    top_10_features.append((feature_name, lg_importances[feature_index]))\n",
    "\n",
    "lg_top_10_df = pd.DataFrame(top_10_features, columns=['Feature', 'Importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM feature importance\n",
    "px.pie(lg_top_10_df, names='Feature', values='Importance', title='Light GBM Feature Importance', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_4_'></a>[Gradient Boost Pipeline Feature Importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost pipeline feature importance\n",
    "pipe_gb.fit(X_train, y_train)\n",
    "\n",
    "gb_classifier = pipe_gb.named_steps['gb_classifier']\n",
    "gb_importances = gb_classifier.feature_importances_\n",
    "gb_indices = np.argsort(gb_importances)[::-1]\n",
    "\n",
    "top_10_features = []\n",
    "for f in range(10):\n",
    "    feature_index = gb_indices[f]\n",
    "    feature_name = X_train.columns[feature_index]\n",
    "    top_10_features.append((feature_name, gb_importances[feature_index]))\n",
    "\n",
    "gb_top_10_df = pd.DataFrame(top_10_features, columns=['Feature', 'Importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb feature importance\n",
    "px.pie(gb_top_10_df, names='Feature', values='Importance', title='Gradient Boost Feature Importance', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_5_'></a>[CatBoost Pipeline Feature Importance](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat Boost pipeline feature importance\n",
    "pipe_cat.fit(X_train, y_train)\n",
    "\n",
    "cat_classifier = pipe_cat.named_steps['cat_classifier']\n",
    "cat_importances = cat_classifier.feature_importances_\n",
    "cat_indices = np.argsort(cat_importances)[::-1]\n",
    "\n",
    "top_10_features = []\n",
    "for f in range(10):\n",
    "    feature_index = cat_indices[f]\n",
    "    feature_name = X_train.columns[feature_index]\n",
    "    top_10_features.append((feature_name, cat_importances[feature_index]))\n",
    "\n",
    "cat_top_10_df = pd.DataFrame(top_10_features, columns=['Feature', 'Importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat boost feature importance\n",
    "px.pie(cat_top_10_df, names='Feature', values='Importance', title='Cat Boost Feature Importance', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_2_'></a>[Comparing Models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series of model scores\n",
    "data = {'Ada Boost': 0.87, 'XG Boost': 0.93 , 'LGBM': 0.92, 'Gradient Boost': 0.91, 'Cat Boost': 0.92, 'Voting Classifier': 0.93 }\n",
    "comp = pd.Series(data, name='Auc-Roc Score')\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model scores\n",
    "px.scatter(comp, color=comp.index, size=comp, title='Model Comparison', symbol=comp, labels={'index': 'Model', 'value': 'AUC-ROC Score'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Final Model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "\n",
    "class1 = AdaBoostClassifier(n_estimators=400, random_state=19, learning_rate=0.1)\n",
    "class2 = GradientBoostingClassifier(n_estimators=1500, learning_rate=0.1, random_state=19, max_depth=5)\n",
    "class3 = XGBClassifier(learning_rate=0.5, max_depth=5, n_estimators=1500) \n",
    "class4 = lgb.LGBMClassifier(objective='binary', metric='auc', boosting_type='gbdt', learning_rate=0.1, max_depth=20, n_estimators=350, num_leaves=30, random_state=19) \n",
    "class5 = CatBoostClassifier(task_type='GPU', loss_function='Logloss', eval_metric='AUC', iterations=1500, random_seed=19)\n",
    " \n",
    "\n",
    "final = VotingClassifier(estimators=[('ada', class1), \n",
    "                                #('gbr', class2), \n",
    "                                ('xgb', class3), \n",
    "                                ('lgb', class4), \n",
    "                                ('cat', class5)], \n",
    "                                verbose=1, voting='soft')\n",
    "final = final.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "final_predictions = final.predict(X_test)\n",
    "\n",
    "result = roc_auc_score(y_test, final_predictions)\n",
    "print()\n",
    "print(\"Voting Classifier model on the test set: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_1_'></a>[Final Model Evaluation Metric](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display AUC ROC\n",
    "probabilities_valid = final.predict_proba(X_test)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "\n",
    "auc_roc = roc_auc_score(y_test, probabilities_one_valid)\n",
    "\n",
    "print(auc_roc)\n",
    "\n",
    "# ROC AUC curve of results\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probabilities_one_valid)\n",
    "\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=800, height=600\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_3_2_'></a>[Final Model Feature Importances](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting feature importance of each model in voting classifier\n",
    "def compute_feature_importance(voting_clf, weights):\n",
    "    \"\"\" Function to compute feature importance of Voting Classifier \"\"\"\n",
    "    \n",
    "    feature_importance = dict()\n",
    "    for est in voting_clf.estimators_:\n",
    "        feature_importance[str(est)] = est.feature_importances_\n",
    "    \n",
    "    fe_scores = [0]*len(list(feature_importance.values())[0])\n",
    "    for idx, imp_score in enumerate(feature_importance.values()):\n",
    "        imp_score_with_weight = imp_score*weights[idx]\n",
    "        fe_scores = list(np.add(fe_scores, list(imp_score_with_weight)))\n",
    "    return fe_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature importance dataframe\n",
    "fedf = pd.DataFrame()\n",
    "fedf['Feature'] = X_train.columns\n",
    "fedf['Feature Importance'] = compute_feature_importance(final, [1, 1, 1, 1])\n",
    "fedf = fedf.sort_values('Feature Importance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting classifier feature importance\n",
    "px.pie(fedf, names='Feature', values='Feature Importance', title='Top 10 Features', template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_4_'></a>[Overall Conclusions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We succeeded in achieving our target AUC ROC score of 0.75. Our final score with the test set is 0.9409, which is roughly 25% more than our target. The boosting models generally had good scores with the training set, but to ensure a better score with the test set, we used a voting classifier. We also looked into the feature importance of the various models in our pipeline, and finally, the important features in our final model. We see that total charges and monthly charges are the most important features, followed by the time frame of the account opening with the year and month. It is reasonable to conclude features such as charges and the length of time a customer has had service, outweigh other factors such as demographics.\n",
    "\n",
    "Therefore, Interconnect can utilize this model to predict churn. Alongside marketing strategies, the company can target customers likely to leave, and therefore implement strategies to prevent churn. They may also be interested in looking at trends in the charges of customers alongside the length of service, to periodically implement marketing strategies to prevent churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All steps of the final work plan were performed, yet we could have done without the K Means analysis, as the results of that model were difficult to interpret. \n",
    "\n",
    "Since we used a voting classifier as our final model, we found it difficult to illustrate the inner workings of this black box model. Therefore, we did extensive research to write a code that could extract the feature importance of each model in the ensemble, and give the average feature importance as the overall summary of the model. \n",
    "\n",
    "Key steps to a successful model and prediction were parsing the start dates into months and years. Furthermore, scaling the dataset and using SMOTE in the pipeline likely helped to achieve a good AUC-ROC score. The final model is a voting classifier with an AUC-ROC score of 0.9409. Choosing the voting classifier was a good decision, yet and XG Boost classifier may have been equally as good. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4f2b0cfe12c109b58467a02dc33230d9e6228c23b43020d2941c37e2a7dfdd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
